
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="viewport" content="width=500">
  <link href="stylesheet.css" rel="stylesheet" type="text/css">
  <title>Thao Ha</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="100%" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <p align="center">
                <name>Thao Ha</name>
              </p>
              <p align="center">
              I graduated from the PhD programme at the <a href="https://www.sutd.edu.sg"> Singapore University of Technology and Design (SUTD) </a> in 2022. 
              During my PhD journey, I was under the supervision of <a href="http://www.cvai.cs.uni-frankfurt.de/team.html">Prof. Gemma Roig</a> (in Goethe University Frankfurt, Germany) and <a href="https://dorienherremans.com">Prof. Dorien Herremans</a> (in SUTD). 
              </p>
              <p align=center>
                <a href="mailto:ivyha010@gmail.com">Email</a> &nbsp/&nbsp
                <a href="#publications">Publications</a> 
              </p>
            </td>
          </tr>
        </table>
        <hr>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading id="interests">Research Interests</heading>
              <p>
              My research interests include computer vision, affective computing, and multimodal representation learning. 
              </p>
              <p>
              In addition to doing research, I also enjoy participating in startup projects.
              </p>
            </td>
          </tr>
        </table>
      <hr>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="100%" valign="middle">
            <heading id="publications">Publications </heading>
          </td>
        </tr>


      </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="35">
          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()" bgcolor="#F0F8FF">   
            <td width="25%">
                <div class="one">
                  <div class="two" id='mpi_image'><img src='images/DS_snapshot.png' width="180"></div>
                  <img src='images/DS_snapshot.png' width="180">
                </div>
                <script type="text/javascript">
                  function mpi_start() {
                    document.getElementById('mpi_image').style.opacity = "1";
                  }

                  function mpi_stop() {
                    document.getElementById('mpi_image').style.opacity = "0";
                  }
                  mpi_stop()
                </script>
              </td>


            <td valign="middle" width="75%">
              <papertitle>EmoMV: Affective Music-Video Correspondence Learning Datasets for Classification and Retrieval</papertitle>
              <br>
              <strong>Thao Ha </strong>,
              <a href="http://www.cvai.cs.uni-frankfurt.de/team.html">Gemma Roig</a>,
              <a href="https://dorienherremans.com">Dorien Herremans</a>
              <br>
              <br>
              <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4189323">Preprint SSRN</a>/
              <a href="https://zenodo.org/record/7011072#.YzRq6nZBxdg">Dataset</a>/
              <a href="https://github.com/ivyha010/EmoMV">Code</a>
              <p></p>
              <p>We construct a collection of three datasets (called EmoMV) for affective correspondence learning between music and video modalities. A benchmark deep neural network model for binary affective music-video correspondence classification is also proposed. This proposed benchmark model is then modified to adapt to affective music-video retrieval.</p>
            </td>
          </tr>

          <tr onmouseout="mpi_stop()" onmouseover="mpi_start()" bgcolor="#F0F8FF">
                <td width="25%">
                <div class="one">
                  <div class="two" id='porlight_image'><img src='images/overview_video_audio_text.png' width="180" height="90"></div>
                  <img src='images/overview_video_audio_text.png' width="180" height="90">
                </div>
                <script type="text/javascript">
                  function porlight_start() {
                    document.getElementById('porlight_image').style.opacity = "1";
                  }

                  function porlight_stop() {
                    document.getElementById('porlight_image').style.opacity = "0";
                  }
                  porlight_stop()
                </script>
              </td>
              <td valign="middle" width="75%">
                  <papertitle>AttendAffectNet â€“ Emotion Prediction of Movie Viewers Using Multimodal Fusion with Self-attention</papertitle>
                <br>
                <strong>Thao Ha</strong>,
                <a href="https://www.researchgate.net/profile/Balamurali-B-T">Balamurali B.T</a>,
                <a href="http://www.cvai.cs.uni-frankfurt.de/team.html">Gemma Roig</a>,
                <a href="https://dorienherremans.com">Dorien Herremans</a>
                <br>
                <em>Sensors</em>, 2021 &nbsp
                <br>
                <br>
                <a href="https://pubmed.ncbi.nlm.nih.gov/34960450/">Paper</a>/
                <a href="https://github.com/ivyha010/AttendAffectNet">Code</a>
                <p></p>
                <p> This is an extended version of the paper "AttendAffectNet: Self-Attention based Networks for Predicting Affective Responses 
                  from Movies". In addition to visual and audio features, we also use features extracted from video subtitles as the model input. Extensive experiments are also carried out.
                </p>
              </td>
            </tr>

          <tr onmouseout="mpi_stop()" onmouseover="mpi_start()" bgcolor="#F0F8FF">
              <td width="25%">
                <div class="one">
                  <div class="two" id='mpi_image'><img src='images/general_scheme.png' width="180" height="70"></div>
                  <img src='images/general_scheme.png' width="180" height="70">
                </div>
                <script type="text/javascript">
                  function mpi_start() {
                    document.getElementById('mpi_image').style.opacity = "1";
                  }

                  function mpi_stop() {
                    document.getElementById('mpi_image').style.opacity = "0";
                  }
                  mpi_stop()
                </script>
              </td>
              <td valign="middle" width="75%">
                  <papertitle>AttendAffectNet: Self-Attention based Networks for Predicting Affective Responses from Movies</papertitle>
                <br>
                <strong>Thao Ha</strong>,
                <a href="https://www.researchgate.net/profile/Balamurali-B-T">Balamurali B.T</a>,
                <a href="https://dorienherremans.com">Dorien Herremans</a>,
                <a href="http://www.cvai.cs.uni-frankfurt.de/team.html">Gemma Roig</a>
                <br>
                <em>ICPR, 2020</em>
                <br>
                <br>
                <a href="https://arxiv.org/ftp/arxiv/papers/2010/2010.11188.pdf">Paper</a>
                <p></p>
                <p>We develop a deep neural network namely AttendAffectNet for predicting emotions of movie viewers 
                  from different input modalities (video, audio) by using the self-attention mechanism.</p>
              </td>
            </tr>

          <tr onmouseout="mpi_stop()" onmouseover="mpi_start()" bgcolor="#F0F8FF">
              <td width="25%">
                <div class="one">
                  <div class="two" id='mpi_image'><img src='images/RGB_OF_Audio_LSTM_upgraded.png' width="180"></div>
                  <img src='images/RGB_OF_Audio_LSTM_upgraded.png' width="180">
                </div>
                <script type="text/javascript">
                  function mpi_start() {
                    document.getElementById('mpi_image').style.opacity = "1";
                  }

                  function mpi_stop() {
                    document.getElementById('mpi_image').style.opacity = "0";
                  }
                  mpi_stop()
                </script>
              </td>
              <td valign="middle" width="75%">
                  <papertitle>Multimodal Deep Models for Predicting Affective Responses Evoked by Movies</papertitle>
                <br>

                <strong>Thao Ha</strong>, 
                <a href="https://dorienherremans.com">Dorien Herremans</a>, 
                <a href="http://www.cvai.cs.uni-frankfurt.de/team.html">Gemma Roig</a>
                <br>
                <em>CVPM</em>, 2019 &nbsp
                <br>
                <br>
                <a href="https://arxiv.org/abs/1909.06957">Paper</a> /
                <a href="https://github.com/ivyha010/emotionprediction">Code</a>
                <p></p>
                <p> We develop and analyze multimodal models for predicting experienced affective responses of viewers watching movie clips.
                The first model is based on fully connected layers without memory on the time component, while the second one incorporates the sequential dependency with a long shortterm memory recurrent neural network (LSTM).   </p>
              </td>
            </tr>


        </table>
        <hr>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="5%"><img src="images/SUTD_logo.png" width="36"></td>
            <td width="95%" valign="center">
              <p>TA, Introduction to Probability and Statistics, 2018, SUTD</p>
            </td>
          </tr>
          <tr>
            <td width="5%"><img src="images/SUTD_logo.png" width="36"></td>
            <td width="95%" valign="center">
              <p>TA, Artificial Intelligence, 2019, SUTD</p>
            </td>
          </tr>

      </td>
    </tr>
  </table>
</body>

</html>
